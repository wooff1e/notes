### CUDA ###
check for cuda-capable gpu:
lspci | grep -i nvidia

architecture:
uname -m

linux info:
cat /etc/*release

cuda toolkit:
https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=deb_local


// a grid of 16 threadblocks of size 256 with parameters a,b,c
// with shared memory 1024 bytes, launching in stream 0 (default)
KernelTest<<<16,256,256*sizeof(int),0>>>(a,b,c)

dim3 block1d(256);
dim3 block2d(16,16);
dim3 block3d(4,8,8);

KernelBlockLiteral<<<16,64>>>(d_a);
KernelBlockDim<<<16, block1d>>>(d_a); // same as prev
KernelBlock2d<<<16, block2d>>>(d_a); // 2d grid
KernelBlock2d<<<16, block3d>>>(d_a); // 3d grid

// thread index within a threadblock
int tidx = threadIdx.x;
int tidy = threadIdx.y; 

// unique thread index within a grid
int tidx = threadIdx.x * blockDim.x + threadIdx.x;

// grid dimensions
int gridWidth = gridDim.x*blockDim.x;
int gridDepth = gridDim.z*blockDim.z;

threads are executed in warps
the warp size is fixed for a specific cuda architecture
conditional blocks if(condition) {} else {} will be executed by ALL threads and then
the output of threads that dont meet the condition is masked out!!
--> reduce thread divergence in a wrap to optimize

Visual Studio Nsight can profile the ratio of Active warps : Stalled warps
