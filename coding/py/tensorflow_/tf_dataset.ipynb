{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pathlib\n",
    "import scipy.ndimage as ndimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a Dataset from data in memory:<br>\n",
    "`tf.data.Dataset.from_tensors()`<br>\n",
    "`tf.data.Dataset.from_tensor_slices()   # from any tensor-like object (numpy array, list, dictionary)`<br>\n",
    "\n",
    "Construct from a .tfrec file (for large datasets that do not fit in memory): <br>\n",
    "`tf.data.TFRecordDataset()`<br>\n",
    "\n",
    "Construct from generator:<br>\n",
    "`tf.data.Dataset.from_generator()`<br>\n",
    "\n",
    "Construct from text files: <br>\n",
    "`tf.data.TextLineDataset()` <br>\n",
    "\n",
    "Construct from CSV file: <br>\n",
    "`tf.data.experimental.make_csv_dataset()`<br>\n",
    "`tf.data.experimental.CsvDataset` <-- lower-level for finer grained control<br>\n",
    "or (if the dataset fits into memory) you can consume file with pandas and then use:<br> \n",
    "`tf.data.Dataset.from_tensor_slices(dict(df))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Dataset object is a Python iterable --> use a for loop\n",
    "for elem in dataset:\n",
    "  print(elem.numpy())\n",
    "\n",
    "# or python iterator\n",
    "it = iter(dataset)\n",
    "print(next(it).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset structure\n",
    " - Each element is the same nested structure of components.<br>\n",
    " Eg. tuple, dict, NamedTuple OrderedDict **(but NOT list!)**\n",
    " - Each component can be any type representable by tf.TypeSpec <br>\n",
    " (tf.Tensor, tf.sparse.SparseTensor, tf.RaggedTensor, tf.TensorArray, or tf.data.Dataset).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random.uniform([4]),\n",
    "    tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "\n",
    "dataset2.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset containing a sparse tensor.\n",
    "dataset3 = tf.data.Dataset.from_tensors(tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]))\n",
    "dataset3.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use value_type to see the type of value represented by the element spec\n",
    "dataset3.element_spec.value_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "per-element transformations, eg.: <br>\n",
    "`Dataset.map()` <br>\n",
    "\n",
    "multi-element transformations, eg.: <br>\n",
    "`Dataset.batch()`\n",
    "\n",
    "reduce all elements to a single result, eg.: <br>\n",
    "`print(dataset.reduce(0, lambda state, value: state + value).numpy())`\n",
    "\n",
    "### map() \n",
    "Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels from file path:\n",
    "'''\n",
    "b'/home/kbuilder/.keras/datasets/flower_photos/daisy/2045022175_ad087f5f60_n.jpg'\n",
    "b'/home/kbuilder/.keras/datasets/flower_photos/tulips/19425920580_cdc8f49aed_n.jpg'\n",
    "b'/home/kbuilder/.keras/datasets/flower_photos/dandelion/160456948_38c3817c6a_m.jpg'\n",
    "'''\n",
    "\n",
    "flowers_root = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True)\n",
    "    \n",
    "flowers_root = pathlib.Path(flowers_root)\n",
    "\n",
    "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\n",
    "\n",
    "def preprocess(file_path):\n",
    "  parts = tf.strings.split(file_path, os.sep)\n",
    "  label = parts[-2]\n",
    "  \n",
    "  image = tf.io.read_file(file_path)\n",
    "  image = tf.io.decode_jpeg(image)\n",
    "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "  image = tf.image.resize(image, [128, 128])\n",
    "  return image, label\n",
    "\n",
    "flower_ds = list_ds.map(preprocess)\n",
    "\n",
    "def show(image, label):\n",
    "  plt.figure()\n",
    "  plt.imshow(image)\n",
    "  plt.title(label.numpy().decode('utf-8'))\n",
    "  plt.axis('off')\n",
    "\n",
    "for image, label in flower_ds.take(2):\n",
    "  show(image, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For performance reasons, use TensorFlow operations for preprocessing your data whenever possible. However, it is sometimes useful to call external Python libraries when parsing your input data. \n",
    "\n",
    "You can use the `tf.py_function()` operation in a `Dataset.map()` transformation.\n",
    "\n",
    "For example, if you want to apply a random rotation, the tf.image module only has tf.image.rot90, which is not very useful for image augmentation. \n",
    "\n",
    "To demonstrate tf.py_function, try using the scipy.ndimage.rotate function instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotate_image(image):\n",
    "  image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)\n",
    "  return image\n",
    "\n",
    "image, label = next(iter(flower_ds))\n",
    "image = random_rotate_image(image)\n",
    "show(image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this function with Dataset.map the same caveats apply as with Dataset.from_generator, you need to describe the return shapes and types when you apply the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_random_rotate_image(image, label):\n",
    "  im_shape = image.shape\n",
    "  [image,] = tf.py_function(random_rotate_image, [image], [tf.float32])\n",
    "  image.set_shape(im_shape)\n",
    "  return image, label\n",
    "\n",
    "rot_ds = flower_ds.map(tf_random_rotate_image)\n",
    "\n",
    "for image, label in rot_ds.take(2):\n",
    "  show(image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: tensorflow_addons has a TensorFlow compatible rotate in tensorflow_addons.image.rotate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_dataset = tf.data.Dataset.range(100)\n",
    "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
    "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
    "batched_dataset = dataset.batch(4)\n",
    "\n",
    "for batch in batched_dataset.take(4):\n",
    "  print([arr.numpy() for arr in batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While tf.data tries to propagate shape information, the default settings of Dataset.batch result in an unknown batch size because the last batch may not be full. Note the Nones in the shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore that last batch, and get full shape propagation:\n",
    "batched_dataset = dataset.batch(7, drop_remainder=True)\n",
    "batched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching tensors with padding for elements of varying size\n",
    "dataset = tf.data.Dataset.range(100)\n",
    "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
    "dataset = dataset.padded_batch(4, padded_shapes=(None,))\n",
    "\n",
    "for batch in dataset.take(2):\n",
    "  print(batch.numpy())\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat() \n",
    "with no arguments this will repeat the input indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch_sizes(ds):\n",
    "  batch_sizes = [batch.shape[0] for batch in ds]\n",
    "  plt.bar(range(len(batch_sizes)), batch_sizes)\n",
    "  plt.xlabel('Batch number')\n",
    "  plt.ylabel('Batch size')\n",
    "\n",
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "titanic_lines = tf.data.TextLineDataset(titanic_file)\n",
    "\n",
    "# repeat 3 times and then batch\n",
    "titanic_batches = titanic_lines.repeat(3).batch(128)\n",
    "plot_batch_sizes(titanic_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch and then repeat 3 times\n",
    "titanic_batches = titanic_lines.batch(128).repeat(3)\n",
    "plot_batch_sizes(titanic_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to perform a custom computation (e.g. to collect statistics) at the end of each epoch then it's simplest to restart the dataset iteration on each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "dataset = titanic_lines.batch(128)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  for batch in dataset:\n",
    "    print(batch.shape)\n",
    "  print(\"End of epoch: \", epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset.shuffle()\n",
    "Maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer.<br>\n",
    "While large buffer_sizes shuffle more thoroughly, they can take a lot of memory, and significant time to fill. Consider using Dataset.interleave() across files if this becomes a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = tf.data.TextLineDataset(titanic_file)\n",
    "counter = tf.data.experimental.Counter()\n",
    "\n",
    "dataset = tf.data.Dataset.zip((counter, lines))\n",
    "dataset = dataset.shuffle(buffer_size=100)\n",
    "dataset = dataset.batch(20)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the buffer_size is 100, and the batch size is 20, the first batch contains no elements with an index over 120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,line_batch = next(iter(dataset))\n",
    "print(n.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset.shuffle doesn't signal the end of an epoch until the shuffle buffer is empty. So a shuffle placed before a repeat will show every element of one epoch before moving to the next. Repeat before a shuffle mixes the epoch boundaries together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.zip((counter, lines))\n",
    "\n",
    "# shuffle --> repeat\n",
    "shuffled = dataset.shuffle(buffer_size=100).batch(10).repeat(2)\n",
    "shuffle_repeat = [n.numpy().mean() for n, line_batch in shuffled]\n",
    "\n",
    "#print item ID's near the epoch boundary\n",
    "print(\"shuffle --> repeat\")\n",
    "for n, line_batch in shuffled.skip(60).take(5):\n",
    "  print(n.numpy())\n",
    "\n",
    "# repeat --> shuffle\n",
    "shuffled = dataset.repeat(2).shuffle(buffer_size=100).batch(10)\n",
    "repeat_shuffle = [n.numpy().mean() for n, line_batch in shuffled]\n",
    "\n",
    "print(\"\\nrepeat --> shuffle\")\n",
    "for n, line_batch in shuffled.skip(55).take(5):\n",
    "  print(n.numpy())\n",
    "\n",
    "\n",
    "plt.plot(shuffle_repeat, label=\"shuffle().repeat()\")\n",
    "plt.plot(repeat_shuffle, label=\"repeat().shuffle()\")\n",
    "plt.ylabel(\"Mean item ID\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "When you have a very inbalanced dataset, you might want to split it into classes and then randomly sample from them to get a balanced representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/download.tensorflow.org/data/creditcard.zip',\n",
    "    fname='creditcard.zip',\n",
    "    extract=True)\n",
    "\n",
    "csv_path = zip_path.replace('.zip', '.csv')\n",
    "\n",
    "creditcard_ds = tf.data.experimental.make_csv_dataset(\n",
    "    csv_path, batch_size=1024, label_name=\"Class\",\n",
    "    # Set the column types: 30 floats and an int.\n",
    "    column_defaults=[float()]*30+[int()])\n",
    "\n",
    "# negative = 99.73%, positive = 0.27%\n",
    "negative_ds = (\n",
    "  creditcard_ds\n",
    "    .unbatch()\n",
    "    .filter(lambda features, label: label==0)\n",
    "    .repeat())\n",
    "positive_ds = (\n",
    "  creditcard_ds\n",
    "    .unbatch()\n",
    "    .filter(lambda features, label: label==1)\n",
    "    .repeat())\n",
    "\n",
    "balanced_ds = tf.data.Dataset.sample_from_datasets(\n",
    "    [negative_ds, positive_ds], [0.5, 0.5]).batch(10)\n",
    "\n",
    "# Now the dataset produces examples of each class with 50/50 probability\n",
    "for features, labels in balanced_ds.take(10):\n",
    "  print(labels.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Dataset.filter works, but results in all the data being loaded twice. To avoid this, you can drop elements to achieve balance instead.\n",
    "\n",
    "class_func passed as an argument, is applied to each dataset element, and is used to determine which class an example belongs to for the purposes of balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_func(features, label):\n",
    "  return label\n",
    "\n",
    "def count(counts, batch):\n",
    "  features, labels = batch\n",
    "  class_1 = labels == 1\n",
    "  class_1 = tf.cast(class_1, tf.int32)\n",
    "\n",
    "  class_0 = labels == 0\n",
    "  class_0 = tf.cast(class_0, tf.int32)\n",
    "\n",
    "  counts['class_0'] += tf.reduce_sum(class_0)\n",
    "  counts['class_1'] += tf.reduce_sum(class_1)\n",
    "\n",
    "  return counts\n",
    "\n",
    "counts = creditcard_ds.take(10).reduce(\n",
    "    initial_state={'class_0': 0, 'class_1': 0},\n",
    "    reduce_func = count)\n",
    "\n",
    "counts = np.array([counts['class_0'].numpy(),\n",
    "                   counts['class_1'].numpy()]).astype(np.float32)\n",
    "\n",
    "fractions = counts/counts.sum() # [0.9973 0.0027]\n",
    "\n",
    "resampler = tf.data.experimental.rejection_resample(\n",
    "    class_func, target_dist=[0.5, 0.5], initial_dist=fractions)\n",
    "\n",
    "# The resampler deals with individual examples, so you must unbatch the dataset before applying the resampler:\n",
    "resample_ds = creditcard_ds.unbatch().apply(resampler).batch(10)\n",
    "\n",
    "# The resampler returns creates (class, example) pairs from the output of the class_func. \n",
    "# In this case, the example was already a (feature, label) pair, \n",
    "# so use map to drop the extra copy of the labels:\n",
    "balanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)\n",
    "\n",
    "# Now the dataset produces examples of each class with 50/50 probability\n",
    "for features, labels in balanced_ds.take(10):\n",
    "  print(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterator Checkpointing\n",
    "In addition to checkpointing the model variables, you can also checkpoint the progress of the dataset iterator. This could be useful if you have a large dataset and don't want to start the dataset from the beginning on each restart. Note however that iterator checkpoints may be large, since transformations such as shuffle and prefetch require buffering elements within the iterator. \n",
    "\n",
    "Note: It is not possible to checkpoint an iterator which relies on external state such as a tf.py_function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_ds = tf.data.Dataset.range(20)\n",
    "\n",
    "iterator = iter(range_ds)\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(0), iterator=iterator)\n",
    "manager = tf.train.CheckpointManager(ckpt, '/tmp/my_ckpt', max_to_keep=3)\n",
    "print([next(iterator).numpy() for _ in range(5)])\n",
    "\n",
    "save_path = manager.save()\n",
    "print([next(iterator).numpy() for _ in range(5)])\n",
    "\n",
    "ckpt.restore(manager.latest_checkpoint)\n",
    "print([next(iterator).numpy() for _ in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf.data with tf.keras\n",
    "`train_ds = tf.data.Dataset.from_tensor_slices((images, labels))`<br>\n",
    "...\n",
    "\n",
    "`model.fit(train_ds, epochs=2)`<br>\n",
    "\n",
    "for infinite dataset:<br>\n",
    "`model.fit(train_ds.repeat(), epochs=2, steps_per_epoch=20)`\n",
    "\n",
    "`loss, accuracy = model.evaluate(train_ds)`<br>\n",
    "\n",
    "For long datasets, set the number of steps to evaluate:<br>\n",
    "`loss, accuracy = model.evaluate(train_ds.repeat(), steps=10)`\n",
    "\n",
    "The labels are not required when predicting:<br>\n",
    "`predict_ds = tf.data.Dataset.from_tensor_slices(images).batch(32)`\n",
    "`result = model.predict(predict_ds, steps = 10)`\n",
    "\n",
    "But if you do pass a dataset containing them, the labels are ignored:\n",
    "`result = model.predict(train_ds, steps = 10)`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
